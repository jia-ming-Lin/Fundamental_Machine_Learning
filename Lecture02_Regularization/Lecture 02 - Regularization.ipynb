{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Polynomial Curve Fitting Cont. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Recall that we can write the error function compactly in matrix/vector form: \n",
    "\t \\begin{eqnarray} \\nonumber\n",
    "\t \tE(\\mathbf{w}) &=& \\frac{1}{2} \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_n \\\\  x_1^2 & x_2^2 & \\ldots & x_n^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_n^M \\end{array}\\right] - \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)\\\\\n",
    "\t \t& & \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_n \\\\  x_1^2 & x_2^2 & \\ldots & x_n^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_n^M \\end{array}\\right]- \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)^T \\nonumber\n",
    "\t \\end{eqnarray}\n",
    "\n",
    "* Ok, lets look closer at the minimization we discussed last class:\n",
    "\t \\begin{eqnarray}\n",
    "\t\t & & \\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} = 0 = \\frac{1}{2} 2 \\mathbf{X}^T \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T\\\\\n",
    "\t\t & & 0 = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\t\t & & \\mathbf{X}^T\\mathbf{t} = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\\\\n",
    "\t\t & & \\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\t \\end{eqnarray}\n",
    "\twhere $\\mathbf{X}^T= \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_n \\\\  x_1^2 & x_2^2 & \\ldots & x_n^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_n^M \\end{array}\\right]$.\n",
    "\n",
    "* Suppose M = 2 and N = 2: \n",
    "\\begin{eqnarray} \\nonumber\n",
    "\t \tE(\\mathbf{w}) &=& \\frac{1}{2} \\left( \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right]\\right) \\left( \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right]\\right)^T \\nonumber \\\\\n",
    "\t \t&=&  \\frac{1}{2}\\left( \\left[ w_0 + w_1x_1 + w_2x_1^2, w_0 + w_1x_2 + w_2x_2^2 \\right]  - \\left[ t_1, t_2 \\right]\\right) \\nonumber\\\\\n",
    "\t \t& & \\left( \\left[ w_0 + w_1x_1 + w_2x_1^2, w_0 + w_1x_2 + w_2x_2^2 \\right]  - \\left[ t_1, t_2 \\right]\\right)^T \\nonumber\\\\\n",
    "\t \t&=&  \\frac{1}{2}\\left( \\left[ w_0 + w_1x_1 + w_2x_1^2 - t_1, w_0 + w_1x_2 + w_2x_2^2  - t_2\\right] \\right)\\nonumber\\\\\n",
    "\t \t& & \\left( \\left[ w_0 + w_1x_1 + w_2x_1^2 -t_1, w_0 + w_1x_2 + w_2x_2^2 -t_2 \\right] \\right)^T \\nonumber\\\\\n",
    "\t \t&=& \\frac{1}{2}  \\left( \\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)^2 +\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)^2 \\right) \\nonumber\n",
    "\t \\end{eqnarray}\n",
    "* Then, let us work out the derivative with respect to the vector $\\mathbf{w}$\n",
    "\\begin{eqnarray} \\nonumber\n",
    "\t \t\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[\\frac{\\partial E(\\mathbf{w})}{\\partial {w}_0}, \\frac{\\partial E(\\mathbf{w})}{\\partial {w}_1}, \\frac{\\partial E(\\mathbf{w})}{\\partial {w}_2} \\right]^T\\nonumber\\\\\n",
    "\t \t%derivative with w_0\n",
    "\t \t&=& \\left[\\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)  + 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right) \\right), \\right. \\nonumber\\\\\n",
    "\t \t%derivative with w_1\n",
    "\t\t& & \\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)x_1   + 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)x_2 \\right),  \\nonumber\\\\\n",
    "\t \t%derivative with w_2\n",
    "\t\t& & \\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)x_1^2 + \\left. 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)x_2^2 \\right) \\right]^T \\nonumber\\\\\n",
    "\t\t&=& \\frac{1}{2} 2 \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right]\\left(  \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right] \\right)^T\n",
    "\t \\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  As we saw previously, if we have a model that is too complex for our data, we may overfit.  \n",
    "\n",
    "* Suppose you have data that you fit a model to, how do you know if you have overfit and/or underfit?  *What is Cross Validation?*\n",
    "\n",
    "* Lets look back at our polynomial curve fitting code. \n",
    "    * What happens to the weight $\\mathbf{w}$ when we overfit? \n",
    "    * Are we more likely to overfit with more or less data? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "%matplotlib inline \n",
    "\n",
    "def generateUniformData(N, l, u, gVar):\n",
    "\t'''generateUniformData(N, l, u, gVar): Generate N uniformly spaced data points \n",
    "    in the range [l,u) with zero-mean Gaussian random noise with variance gVar'''\n",
    "\t# x = np.random.uniform(l,u,N)\n",
    "\tstep = (u-l)/(N);\n",
    "\tx = np.arange(l+step/2,u+step/2,step)\n",
    "\te = np.random.normal(0,gVar,N)\n",
    "\tt = np.sin(2*math.pi*x) + e\n",
    "\treturn x,t\n",
    "\n",
    "def plotData(x1,t1,x2,t2,x3=None,t3=None,legend=[]):\n",
    "    p1 = plt.plot(x1, t1, 'bo') #plot training data\n",
    "    p2 = plt.plot(x2, t2, 'g') #plot true value\n",
    "    if(x3 is not None):\n",
    "        p3 = plt.plot(x3, t3, 'r') #plot training data\n",
    "\n",
    "    #add title, legend and axes labels\n",
    "    plt.ylabel('t') #label x and y axes\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    if(x3 is None):\n",
    "        plt.legend((p1[0],p2[0]),legend)\n",
    "    else:\n",
    "        plt.legend((p1[0],p2[0],p3[0]),legend)\n",
    "        \n",
    "def fitdata(x,t,M):\n",
    "\t'''fitdata(x,t,M): Fit a polynomial of order M to the data (x,t)'''\t\n",
    "\t#This needs to be filled in\n",
    "\tX = np.array([x**m for m in range(M+1)]).T\n",
    "\tw = np.linalg.inv(X.T@X)@X.T@t\n",
    "\treturn w\n",
    "\n",
    "\n",
    "l = 0\n",
    "u = 1\n",
    "N = 25\n",
    "gVar = .5\n",
    "data_uniform  = np.array(generateUniformData(N, l, u, gVar)).T\n",
    "\n",
    "x1 = data_uniform[:,0]\n",
    "t1 = data_uniform[:,1]\n",
    "\n",
    "x2 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "t2 = np.sin(2*math.pi*x2) #compute the true function value\n",
    "    \n",
    "M = 10\n",
    "w = fitdata(x1,t1,M)\n",
    "x3 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([x3**m for m in range(w.size)]).T\n",
    "t3 = X@w #compute the predicted value\n",
    "\n",
    "plotData(x1,t1,x2,t2,x3,t3,['Training Data', 'True Function', 'Estimated\\nPolynomial'])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two common approaches to avoid overfitting:\n",
    "    1. More data: As you have more and more data, it becomes more and more difficult to ``memorize'' the data and its noise. Often, more data translates to the ability to use a more complex model and avoid overfitting.  However, generally, you need exponentially more data with increases to model complexity.  So, there is a limit to how much this helps.  If you have a very complex model, you need a huge training data set. \n",
    "    2. Regularization: Regularization methods add a penalty term to the error function to discourage overfitting.  These penalty terms encourage small values limiting the ability to overfit.   These penalty terms are a way to trade-off between error and complexity.  \n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E^{\\ast}(\\mathbf{w}) &=& \\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 + \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|^2_2\\\\\n",
    "&=& \\frac{1}{2}\\left(\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left(\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T + \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=& \\frac{1}{2}\\left\\| \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\| \\mathbf{w}\\right\\|_2^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "* *What does each term mean/promote in the minimization? Why does the second term make sense for minimizing complexity?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "E^{\\ast}(\\mathbf{w}) &=& \\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 + \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|^2_2\\\\\n",
    "&=& \\frac{1}{2}\\left(\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left(\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T + \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "& &\\frac{\\partial E^{\\ast}(\\mathbf{w})}{\\partial \\mathbf{w}} = 0 = \\mathbf{X}^T\\left(\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T + \\frac{\\lambda}{2}2 \\mathbf{w}\\\\\n",
    "& & 0 = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{X}^T\\mathbf{t} + \\lambda\\mathbf{w}\\\\\n",
    "& & \\mathbf{X}^T\\mathbf{t} = \\left(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I} \\right)\\mathbf{w}\\\\\n",
    "& & \\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I} \\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* The $l_2$ norm penalty is common (because it works so well mathematically with the least-squares error objective) and, so, has many names: shrinkage, ridge regression, weight decay\n",
    "\n",
    "* *So, what happens when $\\lambda$ is increased? decreased?  Can you think of a way to set $\\lambda$?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitdataReg(x,t,M,la):\n",
    "\t'''fitdata(x,t,M): Fit a polynomial of order M to the data (x,t)'''\t\n",
    "\tX = np.array([x**m for m in range(M+1)]).T\n",
    "\tw = np.linalg.inv(X.T@X+(la*np.identity(M+1)))@X.T@t\n",
    "\treturn w\n",
    "\n",
    "fig = plt.figure();\n",
    "\n",
    "l = 0\n",
    "u = 1\n",
    "N = 10\n",
    "gVar = .25\n",
    "data_uniform  = np.array(generateUniformData(N, l, u, gVar)).T\n",
    "\n",
    "x = data_uniform[:,0]\n",
    "t = data_uniform[:,1]\n",
    "\n",
    "w = fitdataReg(x,t,9,0)\n",
    "print(w)\n",
    "x2 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([x3**m for m in range(w.size)]).T\n",
    "t2 = X@w #compute the predicted value\n",
    "\n",
    "w = fitdataReg(x,t,9,10)\n",
    "print(w)\n",
    "x3 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([x3**m for m in range(w.size)]).T\n",
    "t3 = X@w #compute the predicted value\n",
    "\n",
    "plotData(x,t,x2,t2,x3,t3,['Training Data', 'Est. Polynomial NoReg', 'Est. Polynomial Reg']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Interpretation of Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We looked at the regularization term as a *penalty* term in the objective function.  There is another way to interpret the regularization term as well.  Specifically, there is a *Bayesian* interpretation. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min E^{\\ast}(\\mathbf{w}) &=& \\max -E^{\\ast}(\\mathbf{w})\\\\\n",
    "& =& \\max \\exp \\left\\{ -E^{\\ast}(\\mathbf{w})\\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 - \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|^2_2 \\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\\\\\n",
    "&=& \\max \\prod_{n=1}^N \\exp \\left\\{ -\\frac{1}{2} \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, this is a maximization of the *data likelihood* with a *prior*: $p(\\mathbf{X}|\\mathbf{w})p(\\mathbf{w})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Maximum Likelihood\n",
    "\n",
    "* A *data likelihood* is how likely the data is given the parameter set\n",
    "* So, if we want to maximize how likely the data is to have come from the model we fit, we should find the parameters that maximize the likelihood\n",
    "* A common trick of maximizing the likelihood is to maximize the log likelihood.  Often makes the math much easier.  *Why can we maximize the log likelihood instead of the likelihood and still get the same answer?*\n",
    "* Consider: $\\max \\ln \\exp \\left\\{ -\\frac{1}{2}\\left(y(x_n, \\mathbf{w}) - t_n\\right)^2\\right\\}$ We go back to our original objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Maximum A Posteriori (MAP)\n",
    "\n",
    "* Bayes Rule: $p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$\n",
    "* Consider: $p(\\mathbf{w}|\\mathscr{D}) = \\frac{p(\\mathscr{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathscr{D})}$, i.e., posterior $\\propto$ likelihood $\\times$ prior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
