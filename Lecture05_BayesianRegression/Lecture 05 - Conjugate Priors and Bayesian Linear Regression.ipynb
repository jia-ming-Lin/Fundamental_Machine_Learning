{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Mean vs. Mode vs. Expected Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import multivariate_normal\n",
    "import textwrap\n",
    "import math\n",
    "%matplotlib inline  \n",
    "\n",
    "def sampleMeanEx():\n",
    "\t'''sampleMeanEx()'''\n",
    "\tnSamples = (5, 10, 100, 5000)\n",
    "\ta = 3\n",
    "\tb = 1\n",
    "\tfor i in range(len(nSamples)):\n",
    "\t\tfig = plt.figure()\n",
    "\t\tax = fig.add_subplot(*[1,1,1])\n",
    "\t\tdraws = np.random.gamma(shape=a,scale=b,size=nSamples[i])\n",
    "\t\tmode = (a-1)*b\n",
    "\t\texpectedv = a*b\n",
    "\t\tsamplemean = sum(draws)/len(draws)\n",
    "\t\tx = np.linspace(gamma.ppf(0.001, a, scale=b), gamma.ppf(0.999, a, scale=b), 100)\n",
    "\t\tax.plot(x, gamma.pdf(x, a, scale=b), 'r-', lw=5)\n",
    "\t\tax.scatter(draws, np.zeros(len(draws)), c='k')\n",
    "\t\tmyTitle = 'Num Points: ' + str(nSamples[i]) + ' Mode: ' + str(\"%.2f\"%mode) + ' E[x]: ' + str(\"%.2f\"%expectedv) + ' Sample Mean: ' + str(\"%.2f\"%samplemean)\n",
    "\t\tax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\t\tplt.show()\n",
    "\n",
    "sampleMeanEx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Priors\n",
    "\n",
    "* Last class we mentioned the concept of *conjugate priors*\n",
    "* Two distributions have a conjugate prior relationship when the form of the posterior is the same as the form of the prior. \n",
    "* For example, a Gaussian distribution is a conjugate prior for the mean of a Gaussian as shown in the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "p(\\mu |\\mathbf{X}) &\\propto& p(\\mathbf{X}|\\mu)p(\\mu)\\\\\n",
    "&=& \\prod_{i=1}^N\\mathscr{N}(x_i|\\mu, \\sigma^2)\\mathscr{N}(\\mu|\\mu_0, \\sigma_0^2)\\\\\n",
    "&=& \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2} \\right\\}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left\\{-\\frac{1}{2}\\frac{(\\mu - \\mu_0)^2}{\\sigma_0^2} \\right\\}\\\\\n",
    "&=&  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{ \\sum_{i=1}^N \\left(-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2}\\right) -\\frac{1}{2}\\frac{(\\mu - \\mu_0)^2}{\\sigma_0^2} \\right\\}\\\\\n",
    "&=&  \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\sum_{i=1}^N\\frac{(x_i - \\mu)^2}{\\sigma^2} + \\frac{(\\mu - \\mu_0)^2}{\\sigma_0^2} \\right) \\right\\}\\\\\n",
    "&=&  \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\frac{\\sum_{i=1}^Nx_i^2 - 2\\sum_{i=1}^Nx_i\\mu + \\mu^2N}{\\sigma^2} + \\frac{\\mu^2 - 2\\mu\\mu_0 + \\mu_0^2}{\\sigma_0^2} \\right) \\right\\}\\\\\n",
    "&=&  \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\mu^2\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right) - 2\\mu\\left( \\frac{\\sum_{i=1}^Nx_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}  \\right) +  \\frac{\\sum_{i=1}^Nx_i^2 }{\\sigma^2} + \\frac{ \\mu_0^2}{\\sigma_0^2} \\right) \\right\\}\\\\\n",
    "&=& \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\\left( \\mu^2 - 2\\mu\\left( \\frac{\\sum_{i=1}^Nx_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}  \\right)\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} \\right) \\right\\}\\\\\n",
    "& & \\exp\\left\\{ \\frac{\\sum_{i=1}^Nx_i^2 }{\\sigma^2} + \\frac{ \\mu_0^2}{\\sigma_0^2}  \\right\\} \\nonumber\\\\\n",
    "&=& \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\\left( \\mu - \\left( \\frac{\\sum_{i=1}^Nx_i\\sigma_0^2 + \\mu_0\\sigma^2}{\\sigma^2\\sigma_0^2} \\right)\\left( \\frac{N\\sigma_0^2 + \\sigma^2}{\\sigma^2\\sigma_0^2}\\right)^{-1}  \\right)^2 \\right.\\\\\n",
    "&+& \\left. \\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\n",
    "\\left(\n",
    "\\left( \\frac{\\sum_{i=1}^Nx_i\\sigma_0^2 + \\mu_0\\sigma^2}{\\sigma^2\\sigma_0^2} \\right)\\left( \\frac{N\\sigma_0^2 + \\sigma^2}{\\sigma^2\\sigma_0^2}\\right)^{-1} \\right)^2  \\right\\} \n",
    "\\exp\\left\\{ \\frac{\\sum_{i=1}^Nx_i^2 }{\\sigma^2} + \\frac{ \\mu_0^2}{\\sigma_0^2}  \\right\\} \\nonumber\\\\\n",
    "%\n",
    "&=& \\frac{1}{\\sqrt{2\\pi\\sigma^2}\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left\\{  -\\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\\left( \\mu -  \\frac{\\sum_{i=1}^Nx_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2 + \\sigma^2  }  \\right)^2  \\right\\} \\\\\n",
    "& &\\exp\\left\\{  \\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\n",
    "\\left(\\frac{\\sum_{i=1}^Nx_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2 + \\sigma^2  }\\right)^2 + \\frac{\\sum_{i=1}^Nx_i^2 }{\\sigma^2} + \\frac{ \\mu_0^2}{\\sigma_0^2}  \\right\\} \\nonumber\\\\\n",
    "%\n",
    "&=& C \\exp\\left\\{  -\\frac{1}{2}\\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)\\left( \\mu -  \\frac{\\sum_{i=1}^Nx_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2 + \\sigma^2  }  \\right)^2  \\right\\} \\\\\n",
    "&\\propto& \\mathscr{N}\\left(\\mu\\left|\\frac{\\sum_{i=1}^N x_i\\sigma_0^2 + \\mu_0\\sigma^2}{N\\sigma_0^2 + \\sigma^2  }, \\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1}\\right.\\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, as shown above, the form of the posterior is also a Gaussian distribution.  \n",
    "\n",
    "* There are many conjugate prior relationships, e.g., Bernoulli-Beta, Gaussian-Gaussian, Gaussian-InverseWishart, Multinomial-Dirichlet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "* Look back our polynomial regression: \n",
    "\\begin{equation}\n",
    "\\min E^{\\ast}(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N\\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 + \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|_2^2\n",
    "\\end{equation}\n",
    "This is equivalent to: \n",
    "\\begin{equation}\n",
    "\\max \\prod_{n=1}^N \\exp\\left\\{-\\frac{1}{2}\\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{- \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|_2^2 \\right\\}\n",
    "\\end{equation}\n",
    "* As discussed, the first term is the likelihood and the second term is the prior on the weights\n",
    "*  These are Gaussian distributions:\n",
    "\\begin{equation}\n",
    "\\mathscr{N}(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "* $\\sigma^2$ is the variance OR $\\frac{1}{\\sigma^2}$ is the *precision*\n",
    "* So, as $\\lambda$ gets big, variance gets smaller/tighter.  As $\\lambda$ gets small, variance gets larger/wider. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Previously, we used: \n",
    "\\begin{equation}\n",
    "y = \\sum_{j=0}^M w_j x^j\n",
    "\\end{equation}\n",
    "*  We can extend this, to make it more general and flexible: \n",
    "\\begin{equation}\n",
    "y = \\sum_{j=0}^M w_j \\phi_j(\\mathbf{x})\n",
    "\\end{equation}\n",
    "where $\\phi_j(\\mathbf{x})$ is a *basis function*\n",
    "* For example:\n",
    "    * Basis function we were using previously: $\\phi_j(x) = x^j$ (for univariate $x$)\n",
    "    * Linear Basis Function: $\\phi_j(\\mathbf{x}) = x_j$\n",
    "    * Radial Basis Function: $\\phi_j(\\mathbf{x}) = \\exp\\left\\{ - \\frac{(x - \\mu_j)^2}{2s_j^2}\\right\\}$\n",
    "    * Sigmoidal Basis Function: $\\phi_j(\\mathbf{x}) = \\frac{1}{1 + \\exp \\left\\{ \\frac{\\mathbf{x} - \\mu_j}{s}\\right\\}}$\n",
    "\n",
    "* As before:\n",
    "\\begin{equation}\n",
    "t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon\n",
    "\\end{equation}\n",
    "* However, now: \n",
    "\\begin{equation}\n",
    "y = \\mathbf{w}^T\\boldsymbol{\\Phi}(\\mathbf{x}) = [w_0, w_1, \\ldots, w_M][\\phi_0(\\mathbf{x}), \\phi_1(\\mathbf{x}), \\ldots, \\phi_M(\\mathbf{x})]^T\n",
    "\\end{equation}\n",
    "where $\\epsilon \\sim \\mathscr{N}(\\cdot|0, \\beta^{-1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "p(t | \\mathbf{w}, \\beta) = \\prod_{n=1}^N \\mathscr{N}(t_n | \\mathbf{w}^T\\boldsymbol{\\Phi}(\\mathbf{x}_n), \\beta^{-1})\n",
    "\\end{equation}\n",
    "* So, what is the ``trick'' to use to maximize this? \n",
    "\\begin{eqnarray}\n",
    "\\mathscr{L} = \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E(\\mathbf{w})\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{w}} = \\beta \\sum_{n=1}^N(t_n - \\mathbf{w}^T\\boldsymbol{\\Phi}(\\mathbf{x}_n))\\boldsymbol{\\Phi}(\\mathbf{x}_n)^T = 0 \n",
    "\\end{eqnarray}\n",
    "* This results in:\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{ML} = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} \\right)^{-1}\\boldsymbol{\\Phi}^T\\mathbf{t}\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\Phi} = \\left[ \\boldsymbol{\\Phi}(x_1), \\boldsymbol{\\Phi}(x_2), ...\\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What would you do if you want to include a prior? get the MAP solution?  If assuming zero-mean Gaussian noise, then Regularized Least Squares!\n",
    "\n",
    "### Bayesian Linear Regression\n",
    "\n",
    "* Recall: $E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$ where $\\lambda$ is the trade-off regularization parameter\n",
    "* A simple regularizer (and the one we used previously) is: $E_W(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}$\n",
    "* If we assume zero-mean Gaussian noise: $E_D(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N \\left\\{ t_n - \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_n) \\right\\}^2$\n",
    "* Then, the total error becomes: $\\frac{1}{2}\\sum_{n=1}^N \\left\\{ t_n - \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_n) \\right\\}^2 +  \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}$\n",
    "* We can take the derivative, set it equal to zero and solve for the weights.  When we do, we get:\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \\left( \\lambda \\mathbf{I} + \\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T\\mathbf{t}\n",
    "\\end{equation}\n",
    "* Recall, we can interpret this as:\n",
    "\\begin{eqnarray}\n",
    "\\min_{\\mathbf{w}} E^{\\ast} &=& \\min_{\\mathbf{w}}\\left\\{ E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w}) \\right\\}\\\\\n",
    " &=& \\max_{\\mathbf{w}}\\left\\{ -E_D(\\mathbf{w}) - \\lambda E_W(\\mathbf{w}) \\right\\}\\\\\n",
    " &=& \\max_{\\mathbf{w}}\\exp\\left\\{ -E_D(\\mathbf{w}) - \\lambda E_W(\\mathbf{w}) \\right\\}\\\\\n",
    " &=& \\max_{\\mathbf{w}}\\exp\\left\\{ -E_D(\\mathbf{w})\\right\\} \\exp\\left\\{- \\lambda E_W(\\mathbf{w}) \\right\\}\\\\\n",
    " &\\propto& \\max_{\\mathbf{w}} \\prod_{n=1}^N \\mathscr{N}\\left(t\\left|\\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x}_n),  \\beta\\mathbf{I}\\right.\\right) \\mathscr{N}\\left(\\mathbf{w}\\left|\\mathbf{m}_0, \\mathbf{S}_0\\right.\\right)\\\\\n",
    " &=& \\max_{\\mathbf{w}} p\\left(\\mathbf{t}\\left|\\mathbf{w}, \\mathbf{X}\\right.\\right)p\\left(\\mathbf{w}\\right)\\\\\n",
    " &\\propto& \\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathbf{m}_N, \\mathbf{S}_N)= \\mathscr{N}(\\mathbf{w}|\\mathbf{m}_N, \\mathbf{S}_N)\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{m}_N = \\mathbf{S}_N\\left(\\mathbf{S}_0\\mathbf{m}_0 + \\beta\\boldsymbol{\\Phi}^T\\mathbf{t} \\right)$ and $\\mathbf{S}_N^{-1} = \\mathbf{S}_0^{-1} + \\beta\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}$\n",
    "\n",
    "* What happens with different values of $\\beta$ and $\\mathbf{S}_0$? \n",
    "\n",
    "\n",
    "* To simplify, let us assume that $\\mathbf{S}_0 = \\alpha^{-1}\\mathbf{I}$ and $\\mathbf{m}_0 = \\mathbf{0}$, thus, $\\mathbf{m}_N = \\beta\\mathbf{S}_N\\boldsymbol{\\Phi}^T\\mathbf{t}$ and $\\mathbf{S}_N^{-1} = (\\alpha^{-1}\\mathbf{I})^{-1} + \\beta\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} =  \\alpha\\mathbf{I} + \\beta\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}$\n",
    "* This results in the following Log Posterior: \n",
    "\\begin{eqnarray}\n",
    "\\ln p(\\mathbf{w}|\\mathbf{t}) = - \\frac{\\beta}{2}\\sum_{n=1}^N\\left( t_n - \\mathbf{w}^T\\mathbf{\\Phi}(\\mathbf{x}_n)\\right)^2 - \\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w} + const\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Let us suppose we are dealing with 1-D data, $\\mathbf{X} = \\left\\{ x_1, \\ldots, x_N \\right\\}$ and a linear form for y: $y(x, \\mathbf{w}) = w_0 + w_1x$\n",
    "\n",
    "* We are going to generate synthetic data from: $t = -0.3 + 0.5x + \\epsilon$ where $\\epsilon$ is from zero-mean Gaussian noise. The goal is to estimate the true values $w_0 = -0.3$ and $w_1 = 0.5$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerBasisFunc(dataX):\n",
    "\treturn np.vstack([dataX[i,:]@dataX[i,:].T for i in range(dataX.shape[0])])\n",
    "\n",
    "def prodBasisFunc(dataX):\n",
    "\treturn np.vstack([dataX[i,0]*dataX[i,1] for i in range(dataX.shape[0])])\n",
    "\n",
    "def xorExample():\n",
    "\t'''xorExample()'''\n",
    "\tclass1X = np.vstack([np.array([-1,-1])+np.random.normal(0,.1,2 ) for i in range(100)])\n",
    "\tclass1X = np.vstack((class1X,np.vstack([np.array([1,1])+np.random.normal(0,.1,2 ) for i in range(100)])))\n",
    "\tclass2X = np.vstack([np.array([1,-1])+np.random.normal(0,.1,2 ) for i in range(100)])\n",
    "\tclass2X = np.vstack((class2X,np.vstack([np.array([-1,1])+np.random.normal(0,.1,2 ) for i in range(100)])))\n",
    "\tphi1X = prodBasisFunc(class1X)\n",
    "\tphi2X = prodBasisFunc(class2X)\n",
    "    \n",
    "\tfig = plt.figure()\n",
    "\tax = fig.add_subplot(*[1,2,1])\n",
    "\tax.scatter(class1X[:,0], class1X[:,1], c='r') \n",
    "\tax.scatter(class2X[:,0], class2X[:,1]) \n",
    "\tax = fig.add_subplot(*[1,2,2])\n",
    "\tax.scatter(phi1X, np.zeros(phi1X.shape)+0.001, c='r') \n",
    "\tax.scatter(phi2X, np.zeros(phi2X.shape)) \n",
    "\tplt.show()\n",
    "    \n",
    "xorExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_prior_func():   \n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    #set up variables\n",
    "    a = -0.3\n",
    "    b = 0.5\n",
    "    rangeX = [-1, 1]\n",
    "    step = 0.025\n",
    "    X = np.mgrid[rangeX[0]:rangeX[1]:step]\n",
    "    alpha = 30\n",
    "    beta = 2\n",
    "    S0 = (1/alpha)*np.eye(2)\n",
    "    draw_num = (0,1,2,3,20)\n",
    "\n",
    "    #initialize prior/posterior and sample data\n",
    "    sigma = S0\n",
    "    mean = [0,0]\n",
    "    draws = np.random.uniform(rangeX[0],rangeX[1],size=draw_num[-1])\n",
    "    T = a + b*draws + np.random.normal(loc=0, scale=math.sqrt(1/beta))\n",
    "\n",
    "    for i in range(len(draw_num)):\n",
    "        if draw_num[i]>0: #skip first image\n",
    "            #Show data likelihood\n",
    "            Phi = np.vstack((np.ones(draws[0:draw_num[i]].shape), draws[0:draw_num[i]]))\n",
    "            t = T[0:draw_num[i]]\n",
    "            sigma = np.linalg.inv(S0 + beta*Phi@Phi.T)\n",
    "            mean = beta*sigma@Phi@t\n",
    "\n",
    "            w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "            p = multivariate_normal(t[draw_num[i]-1], 1/beta)\n",
    "            out = np.empty(w0.shape)\n",
    "            for j in range(len(w0)):\n",
    "                out[j] = p.pdf(w0[j]+w1[j]*draws[draw_num[i]-1])\n",
    "\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+1])\n",
    "            ax.pcolor(w0, w1, out)\n",
    "            ax.scatter(a,b, c='c')\n",
    "            myTitle = 'data likelihood'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        #Show prior/posterior\n",
    "        w0, w1 = np.mgrid[rangeX[0]:rangeX[1]:step, rangeX[0]:rangeX[1]:step]\n",
    "        pos = np.empty(w1.shape + (2,))\n",
    "        pos[:, :, 0] = w0; pos[:, :, 1] = w1\n",
    "        p = multivariate_normal(mean, sigma)\n",
    "\n",
    "        ax = fig.add_subplot(*[len(draw_num),3,(i)*3+2])\n",
    "        ax.pcolor(w0, w1, p.pdf(pos))\n",
    "        ax.scatter(a,b, c='c')\n",
    "        myTitle = 'Prior/Posterior'\n",
    "        ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "        #Show data space\n",
    "        for j in range(6):\n",
    "            w0, w1 = np.random.multivariate_normal(mean, sigma)\n",
    "            t = w0 + w1*X\n",
    "            ax = fig.add_subplot(*[len(draw_num),3,(i)*3+3])\n",
    "            ax.plot(X,t)\n",
    "            if draw_num[i] > 0:\n",
    "                ax.scatter(Phi[1,:], T[0:draw_num[i]])\n",
    "            myTitle = 'data space'\n",
    "            ax.set_title(\"\\n\".join(textwrap.wrap(myTitle, 100)))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "likelihood_prior_func()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
