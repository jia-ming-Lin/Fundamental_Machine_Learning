{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. General Pattern Recognition Mode of Operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A common overall approach for pattern recognition and machine learning:\n",
    "\t* Collect training data relevant to your problem/application\n",
    "\t* Preprocess the training data, extract features, preprocess/normalize the Features\n",
    "\t* Assume a model for your problem and the associated features\n",
    "\t* Train the model by estimating model parameters \n",
    "\t* Collect test data, apply same preprocessing and feature extraction, apply the trained model to test data\n",
    "* Many \"tricks\" in making the above work well\n",
    "* Of course, this is not the only way.  Just a very common approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"open_source_trainingoverview.png\" alt=\"Overview of Training\" style=\"width: 8000px;\"/>\n",
    "\n",
    "<img src=\"learningalg.png\" style=\"width: 500px;\"/>\n",
    "(Fig. modeled after the one in J. Principe's text)\n",
    "\n",
    "<img src=\"open_source_testingoverview.png\" alt=\"Overview of Training\" style=\"width: 8000px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One example:  Suppose you would like to estimate someone's age from a photograph.  \n",
    "\t* Collect training data by collecting photographs of individuals and labeling each photo with the age of the individual\n",
    "\t* Preprocess the imagery (e.g., by downsampling or cropping each image to be the same size), Extract features (e.g., face shape/contour descriptors, facial hair indicator, etc.. ) \n",
    "\t* Assume that a neural network is an appropriate model.\n",
    "\t* Train the neural network using backpropagation\n",
    "\t* When given incoming test data, extract features in same manner and determine age using the trained neural network. \n",
    "* You could also assume a \"dictionary learning\" model instead of a neural network or you could assume a convolutional neural network model with the raw pixel values as features\n",
    "* You could extract other features\n",
    "* You could train your model using MCMC sampling instead of backpropagation\n",
    "* and the options go on and on... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Polynomial Curve Fitting Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets begin by considering the polynomial curve fitting example in the first chapter of the text. \n",
    "* Suppose we have a training set with $N$ data samples, $\\mathbf{x} = (x_1, x_2, \\ldots, x_N)^T$ and corresponding desired outputs $\\mathbf{t} = (t_1, t_2, \\ldots, t_N)^T$ where sample $x_i$ has the desired label $t_i$\n",
    "\n",
    "* We generally organize data into *vectors* and *matrices*. Not only is it a common way to organize the data, but it allows us to easily apply linear algebraic operations during analysis.\n",
    "\n",
    "## Let us review some linear algebra:\n",
    "  * What is a vector?\n",
    "  \\begin{equation} \\mathbf{x} = \\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_D\\end{array} \\right] \\end{equation}\n",
    "  \n",
    "  * What is the transpose operation? \n",
    "    \\begin{equation}\n",
    "\t\t \\mathbf{x}^T = \\left[  x_1,  x_2 , \\cdots , x_D \\right]\n",
    "         \\end{equation}\n",
    "         \n",
    "  * Vectors are often used to represent a data point (as a concatenation of all of the associated features for the data point)\n",
    "  * Given a vector $\\mathbf{x} \\in \\mathbb{R}^D$ and a scalar value $a$, *what is $a\\mathbf{x}$?*  *What does this operation do geometrically?* \n",
    "  * Given $\\mathbf{x} \\in \\mathbb{R}^D$ and $\\mathbf{y} \\in \\mathbb{R}^D$, *what is $\\mathbf{x} + \\mathbf{y}$?  What is the geometric interpretation?*\n",
    "  * Given $\\mathbf{x} \\in \\mathbb{R}^D$ and $\\mathbf{y} \\in \\mathbb{R}^D$, *what is $\\mathbf{x} - \\mathbf{y}$? What is the geometric interpretation?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = 2\n",
    "x = np.array([[1,2,3]])\n",
    "y = np.array([[4],[5],[6]])\n",
    "print('x:',x)\n",
    "print('x.T:',x.T)\n",
    "print('a*x:', a*x)\n",
    "print('x+y.T:', x+y.T)\n",
    "print('y-x.T:',y-x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall: $\\left(\\mathbf{A}^T\\mathbf{B}\\right)^T = \\mathbf{B}^T\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* An important operation is the *inner product*:\n",
    "$\\mathbf{x}^T\\mathbf{y} = \\mathbf{y}^T\\mathbf{x} = \\sum_{i=1}^D x_iy_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(x.dot(y))\n",
    "print(x@y)\n",
    "print(np.inner(x,y.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the *outer product*? $xy^\\top \\!=\\! \\left[\\!\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_d\\end{array}\\!\\right]\\!\\!\n",
    "\\left[\\!\\begin{array}{c}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\\end{array}\\!\\right]^\\top \\!\\!=\\! \\left[\\!\\begin{array}{c c c c}\n",
    "x_1y_1 & x_1y_2 & \\cdots & x_1y_n\\\\\n",
    "x_2y_1 & x_2y_2 & \\cdots & x_2y_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_dy_1 & x_dy_2 & \\cdots & x_dy_n\\end{array}\\!\\right]\\! \\!\\in\\! \\mathcal{R}^{d \\times n}.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.outer(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.outer(x,y).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $l_p$ norm is an important concept. Given a vector, $\\mathbf{x}$ and a $p$-value, the $l_p$ norm is defined as:\n",
    "\\begin{eqnarray}\n",
    "\\left\\|\\mathbf{x}\\right\\|_p = \\left( \\sum_{d=1}^D |x_d|^p \\right)^{\\frac{1}{p}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "So, if $p=2$, then the $l_2$ norm of a vector is:\n",
    "\\begin{eqnarray}\n",
    "\\left\\|\\mathbf{x}\\right\\|_2 = \\left( \\sum_{d=1}^D |x_d|^2 \\right)^{\\frac{1}{2}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(np.linalg.norm(x,ord=2))\n",
    "print((x@x)**(1/2))\n",
    "print(np.linalg.norm(x,ord=3))\n",
    "print(np.linalg.norm(x,ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([-1, -2, -3])\n",
    "print(np.linalg.norm(x,ord=2))\n",
    "print((x@x)**(1/2))\n",
    "print(np.linalg.norm(x,ord=3))\n",
    "print(np.linalg.norm(x,ord=1))\n",
    "print(np.linalg.norm(x,ord=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([-1, 0, 3])\n",
    "print(np.linalg.norm(x,ord=2))\n",
    "print((x@x)**(1/2))\n",
    "print(np.linalg.norm(x,ord=3))\n",
    "print(np.linalg.norm(x,ord=1))\n",
    "print(np.linalg.norm(x,ord=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note the notation:  \n",
    "   * scalar values are unbolded (e.g., $N$, $x$)\n",
    "   * vectors are lower case and bolded (e.g., $\\mathbf{x}$)\n",
    "   * matrices are uppercase and bolded (e.g., $\\mathbf{A} \\in \\mathbb{R}^{D \\times N}$)\n",
    "   * vectors are generally assumed to be column vectors (e.g., $\\mathbf{x}^T = \\left(x_1, \\ldots, x_N\\right)$ and $\\mathbf{x} =  \\left(x_1, \\ldots, x_N\\right)^T$ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the Polynomial Curve fitting Example\n",
    "* Suppose the data actually came from some unknown hidden function. \n",
    "* In practice/application, we only have the training data and its corresponding desired values (both of which may be noisy).  From this training data, we want to learn a mapping from input values $x$ to the desired output values $t$. \n",
    "* If we knew the hidden function, we would not need to learn the mapping - we would already know it.  However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have.\n",
    "* We will learn (i.e., train a model to estimate) that mapping from the training data $\\left\\{ \\mathbf{x}, \\mathbf{t} \\right\\}$. \n",
    "* Then, when we are given test data, we can predict each test data point's $t$ value using the mapping that we estimated. \n",
    "\n",
    "* For this problem, we assume that the original data $x$ is sufficient and appropriate (so, we do not need to preprocess or extract features).  Then, we have completed steps 1 and 2 of the general approach listed in Section 0 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we must assume a model.  Lets assume a polynomial function as our model (following the example in the text):  \n",
    "\n",
    "$y(x,\\mathbf{w}) = w_0 + w_1x + w_2x^2 + \\ldots + w_Mx^M = \\sum_{j=0}^M w_jx^j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we must *train* this model by estimating the unknown parameters ($\\mathbf{w}$) that maps the training data, $\\mathbf{x}$, to their desired values, $\\mathbf{t}$, given some assumed value for $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, we have $N$ discrete points from which to estimate $\\mathbf{w}$.  We can minimize the squared error to estimate the parameters:\n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\arg \\min_\\mathbf{w} E(\\mathbf{w}) &=& \\frac{1}{2} \\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n\\right)^2\\\\\n",
    "\t\t&=& \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{j=0}^M w_jx_n^j -t_n \\right)^2\n",
    "\t \\end{eqnarray}\n",
    "* Consider the following illustration of the error function: \n",
    "<img src=\"fig1.jpg\"  style=\"width: 200px;\"/>\n",
    "The red lines correspond to the error between the data and the functional approximation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  We can write the error function compactly in matrix/vector form: \n",
    "\t \\begin{eqnarray} \\nonumber\n",
    "\t \tE(\\mathbf{w}) &=& \\frac{1}{2} \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right] - \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)\\\\\n",
    "\t \t& & \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]- \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)^T \\nonumber\\\\\n",
    "        &=& \\frac{1}{2}  \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T\\\\\n",
    "        &=& \\frac{1}{2}\\left\\| \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T \\right\\|_2^2\n",
    "\t \\end{eqnarray}\n",
    "     where  \n",
    "     \\begin{eqnarray}\\mathbf{X}^T &=& \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]\\\\\n",
    "     &=& \\left[ \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N\\right]\n",
    "     \\end{eqnarray}\n",
    "    and\n",
    "    \\begin{eqnarray}\\mathbf{x}_i = \\left[x_i^0, x_i^1, x_i^2, \\ldots, x_i^M \\right]^T \\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, we want $E(\\mathbf{w})$ to be small.  How do we solve for $\\mathbf{w}$?\n",
    "* We can take the derivative of the error function, set it to zero, and solve for the parameters.  In general, this method does not guarantee that the parameters we estimate are minima of the error function (e.g., may be an inflection point, maxima).  It is a necessary condition (but not sufficient). However, if the function is convex, then it will always find the global optima. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we take the derivative of a function with respect to a vector? \n",
    "\t \\begin{equation*}\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) =\\! \\left[\\frac{\\partial}{\\partial x_1}f(\\mathbf{x}),\\frac{\\partial}{\\partial x_2}f(\\mathbf{x}),\\ldots,\\frac{\\partial}{\\partial x_n}f(\\mathbf{x})\\right]^\\top\\!\\!\\in\\! \\mathcal{R}^{n \\times 1}.\n",
    "\\end{equation*}\n",
    "\n",
    "* So, what would the derivative of $E(\\mathbf{w})$ be with respect to $\\mathbf{w}$? \n",
    "\\begin{eqnarray}\n",
    " E(\\mathbf{w}) &=& \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{j=0}^M w_jx_n^j -t_n \\right)^2\\\\\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[ \\frac{\\partial E(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial E(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial E(\\mathbf{w})}{\\partial w_M} \\right]^T\\\\\n",
    "&=& \\left[ \\sum_{n=1}^N \\left( \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^0 ,  \\sum_{n=1}^N \\left(  \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^1 , \\ldots, \\sum_{n=1}^N \\left(  \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^M  \\right]^T  \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Similarly, \n",
    "\\begin{eqnarray}\n",
    "\t\t  \\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[\\frac{1}{2} 2 \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\mathbf{X}\\right]^T\n",
    "\t \\end{eqnarray}\n",
    "\twhere $\\mathbf{X}^T= \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]$.\n",
    "* Then, we can set the derivative to zero and solve: \n",
    "\\begin{eqnarray}\n",
    "\t\t & & 0 = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\t\t & & \\mathbf{X}^T\\mathbf{t} = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\\\\n",
    "\t\t & & \\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\t \\end{eqnarray}\n",
    "    \n",
    "Similarly, \n",
    "\\begin{eqnarray}\n",
    "\t\t & & 0 =  \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\mathbf{X}\\\\\n",
    "        & & 0 =   \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X}\\\\        \n",
    "        & & \\mathbf{t}^T\\mathbf{X} = \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\\\\n",
    "        & & \\mathbf{t}^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1} = \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\n",
    "        \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\\\\n",
    "         & &\\mathbf{t}^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1} = \\mathbf{w}^T\\\\\n",
    "         & &\\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to data generated from a (noisy) sine curve \n",
    "\n",
    "\n",
    "* Suppose our data actually came from: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is Gaussian zero-mean random noise. \n",
    "* The univariate Gaussian Distribution: \n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} \\exp\\left\\{ - \\frac{1}{2\\sigma^2}(x - \\mu)^2\\right\\}\n",
    "\t\\end{eqnarray}\n",
    "\n",
    "<img src=\"fig2.jpg\"  style=\"width: 200px;\"/>\n",
    "\n",
    "* If the noise is zero-mean Gaussian distributed, it is like we are saying there is a Gaussian around the true curve: \n",
    "\n",
    "<img src=\"fig3.jpg\"  style=\"width: 200px;\"/>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\t t = y + \\epsilon\\\\\n",
    "\t\t \\epsilon = t - y\n",
    "\t \\end{eqnarray}\n",
    "\t where\n",
    "\t \\begin{eqnarray}\n",
    "\t \t\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    " \t \\end{eqnarray}\n",
    " \t thus\n",
    " \t \\begin{eqnarray}\n",
    " \t \t\\mathcal{N}(t-y|0,1) &\\propto& \\exp\\left\\{ -\\frac{1}{2} \\frac{(t-y-0)^2}{1^2} \\right\\}\\\\\n",
    " \t \t&=& \\exp\\left\\{ -\\frac{1}{2} (t-y)^2 \\right\\}\\\\\n",
    " \t \t&=&  \\exp\\left\\{ -E(\\mathbf{w}) \\right\\}\n",
    " \t\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, the squared error objective function, $E(\\mathbf{w})$, assumes Gaussian noise. \n",
    "* Another way to look at it: $t$ is distributed according to a Gaussian distribution with mean $y$\n",
    "\n",
    "* Also, *What is the multivariate Gaussian distribution?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, lets generate data from the *true* underlying function (which, in practice, we would not know)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def generateUniformData(N, l, u, gVar):\n",
    "\t'''generateUniformData(N, l, u, gVar): Generate N uniformly spaced data points \n",
    "    in the range [l,u) with zero-mean Gaussian random noise with variance gVar'''\n",
    "\t# x = np.random.uniform(l,u,N)\n",
    "\tstep = (u-l)/(N)\n",
    "\tx = np.arange(l+step/2,u+step/2,step)\n",
    "\te = np.random.normal(0,gVar,N)\n",
    "\tt = np.sin(2*math.pi*x) + e\n",
    "\treturn x,t\n",
    "\n",
    "l = 0\n",
    "u = 1\n",
    "N = 100\n",
    "gVar = .1\n",
    "data_uniform  = np.array(generateUniformData(N, l, u, gVar)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets plot this data and the underlying *true* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "def plotData(x1,t1,x2,t2,x3=None,t3=None,legend=[]):\n",
    "\n",
    "    #plot everything\n",
    "    p1 = plt.plot(x1, t1, 'bo') #plot training data\n",
    "    p2 = plt.plot(x2, t2, 'g') #plot true value\n",
    "    if(x3 is not None):\n",
    "        p3 = plt.plot(x3, t3, 'r') #plot training data\n",
    "\n",
    "    #add title, legend and axes labels\n",
    "    plt.ylabel('t') #label x and y axes\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    if(x3 is None):\n",
    "        plt.legend((p1[0],p2[0]),legend)\n",
    "    else:\n",
    "        plt.legend((p1[0],p2[0],p3[0]),legend)\n",
    "\n",
    "x1 = data_uniform[:,0]\n",
    "t1 = data_uniform[:,1]\n",
    "\n",
    "x2 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "t2 = np.sin(2*math.pi*x2) #compute the true function value\n",
    "    \n",
    "plotData(x1, t1, x2, t2,legend=['Training Data', 'True Function'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now lets fit the data using the polynomial curve fitting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitdata(x,t,M):\n",
    "\t'''fitdata(x,t,M): Fit a polynomial of order M to the data (x,t)'''\t\n",
    "\t#This needs to be filled in\n",
    "\tX = np.array([x**m for m in range(M+1)]).T\n",
    "\tw = np.linalg.inv(X.T@X)@X.T@t\n",
    "\treturn w\n",
    "\n",
    "        \n",
    "M = 1\n",
    "w = fitdata(x1,t1,M)\n",
    "xrange = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([xrange**m for m in range(w.size)]).T\n",
    "esty = X@w #compute the predicted value\n",
    "plotData(x1,t1,x2,t2,xrange,esty,['Training Data', 'True Function', 'Estimated\\nPolynomial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting/Overtraining\n",
    "\n",
    "* In the polynomial curve fitting example, $M$ is the *model order*. \n",
    "* As $M$ increases, there are more parameters (more $w$) to learn and, so, the model becomes more complex.  \n",
    "* As a model is more and more complex, it is more likely to *overfit* or *overtrain*.  This essentially means it may \"memorize\" the input training data (including all of the training data's noise).  \n",
    "* Overfitting means that the performance of the model will likely decrease on unknown test data.  Overfitting means that the \"true\" underlying model of the data is not estimated/learned but instead results in a poor representation that memorizes meaningless noise in the data.\n",
    "* There are two common approaches to avoid overfitting:\n",
    "     1. More data: As you have more and more data, it becomes more and more difficult to \"memorize\" the data and its noise. Often, more data translates to the ability to use a more complex model and avoid overfitting.  However, generally, you need exponentially more data with increases to model complexity.  So, there is a limit to how much this helps.  If you have a very complex model, you need a huge training data set. \n",
    "     2. Regularization: Regularization methods add a penalty term to the error function to discourage overfitting.  These penalty terms encourage small values limiting the ability to overfit.  (This is just a teaser. We will discuss this further in the future.)\n",
    "\n",
    "\n",
    "* You can also *underfit* your data.  When you underfit, your model complexity is not complex enough to model all of the complexities in your data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beer Foam Example\n",
    "\n",
    "* Lets go through the Polynomial Curve fitting again with another example\n",
    "* Obtained from: http://www.stat.ufl.edu/~winner/datasets.html \n",
    "\n",
    "Source: A. Leike (2002). \"Demonstration of the Exponential Decay Law Using Beer Froth,\" European Journal of Physics, Vol. 23, #1, pp. 21-26\n",
    "\n",
    "Description: Measurements of wet foam height and beer height at various time points for 3 brands of beer. Author fits exponential decay model: $H(t) = H(0)e^{-\\lambda t}$\n",
    "\n",
    "Variables/Columns:\n",
    "<li> Time from pour (seconds)  4-8\n",
    "<li> Erdinger Weissbier foam height (cm)  10-16\n",
    "<li> Augustinerbrau Munchen foam height (cm)    18-24\n",
    "<li> Budweiser foam height (cm)    26-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "beerData = np.loadtxt('beer_foam.dat.txt')\n",
    "\n",
    "plt.scatter(beerData[:,0], beerData[:,1], color = \"red\")\n",
    "plt.scatter(beerData[:,0], beerData[:,2], color = \"blue\")\n",
    "plt.scatter(beerData[:,0], beerData[:,3], color = \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beerData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Then we can fit the data using the polynomial curve fitting method we derived\n",
    "x = beerData[:,0]\n",
    "t = beerData[:,2]\n",
    "w = fitdata(x,t,M=4)\n",
    "print(w)\n",
    "\n",
    "#Now let us use the weights in test\n",
    "xrange = np.arange(beerData[0,0],beerData[beerData.shape[0]-1,0],0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([xrange**m for m in range(w.size)]).T\n",
    "esty = X@w #compute the predicted value\n",
    "\n",
    "plotData(x,t,xrange,esty,legend=['Training Data','Estimated\\nPolynomial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What will the foam height be at t = ____? \n",
    "t = 500\n",
    "x_test = np.array([t**m for m in range(w.size)]).T\n",
    "print(x_test)\n",
    "predicted_height = x_test@w\n",
    "print(predicted_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Training, Validation, Testing Data Sets\n",
    "\n",
    "* Since we can (often, easily) overfit, our error or prediction performance performance on a training data set is not a good indication of performance on unknown test data.  \n",
    "* One way estimate test performance of a system on unknown test data is to use of the training data for training and some for validation (to act like unknown test data). \n",
    "* If you are repeatedly changing your model/adjusting parameters/tweaking your algorithm, you may even over fit the hold-out validation set.  So, you can hold out yet another set for testing. \n",
    "* However, in general, we only have a limited amount of training data. So, we want to use as much of it as possible for training.  One strategy to balance the tradeoff between needing training data and validation data is to use cross-validation.\n",
    "* Cross-validation can also given an indication of stability/robustness of your method. \n",
    "* However there are downsides to cross-validation: need to train many times (which can sometimes be very computationally complex), you end up with several models - how do you pick the final one to use?\n",
    "\n",
    "* See the Haykin reference (reference details in README) for more reading on cross-validation.  The Haykin Neural Network text is the source these cross-validation notes were motivated/derived from. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment: Read Chapter 3, Due before next class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
