{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Models and, specifically, Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus far, we have primarily discussed relatively simple models consisting of only one peak in the probability distribution function (pdf) when representing data using pdfs.  \n",
    "* For example, when we introduced the probabilistic generative classifier, our examples focused on representing each class using a single Gaussian distribuion. \n",
    "* Consider the following data set, would a multivariate Gaussian be able to represent each of these clustering data set well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets \n",
    "%matplotlib inline\n",
    "\n",
    "n_samples = 1500\n",
    "n_clusters = 1; \n",
    "\n",
    "#generate data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X, y = datasets.make_blobs(n_samples=n_samples , centers = n_clusters)\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "#Plot Results\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Would a single multivariate Gaussian be able to represent this data set well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10; \n",
    "\n",
    "#generate data\n",
    "X, y = datasets.make_blobs(n_samples=n_samples , centers = n_clusters)\n",
    "\n",
    "#Plot Results\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The second data set would be better represented by a *mixture model* \n",
    "\n",
    "$p(x) = \\sum_{k=1}^K \\pi_k f(x | \\theta_k)$\n",
    "\n",
    "where \n",
    "\n",
    "$0 \\le \\pi_k \\le 1$, $\\sum_k \\pi_k =1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If each $f(x | \\theta_k)$ is assumed to be a Gaussian distribution, then the above mixture model would be a *Gaussian Mixture Model*\n",
    "\n",
    "$p(x) = \\sum_{k=1}^K \\pi_k N(x | \\mu_k, \\Sigma_k)$\n",
    "\n",
    "where \n",
    "\n",
    "$0 \\le \\pi_k \\le 1$, $\\sum_k \\pi_k =1$\n",
    "\n",
    "* *How would you draw from samples from a Gaussian Mixture Model? from a mixture model in general?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gaussian mixture models (GMMs) can be used to learn a complex distribution that represents a data set.  Thus, it can be used within the probabilistic generative classifier framework to model complex classes. \n",
    "* GMMs are also commonly used for clustering where a GMM is fit to a data set to be clustered and each estimated Gaussian component is a resulting cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *If you were given a data set, how would you estimate the parameters of a GMM to fit the data?*\n",
    "* A common approach for estimating the parameters of a GMM given data is *expectation maximization* (EM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EM is a general algorithm that can be applied to a variety of problems (not just mixture model clustering).\n",
    "\n",
    "* With MLE, we define a likelihood and maximize it to find parameters of interest. \n",
    "\n",
    "* With MAP, we maximize the posterior to find parameters of interest. \n",
    "\n",
    "* The goal of EM is to also find the parameters that maximize your likelihood function. \n",
    "\n",
    "* *The 1st step* is to define your likelihood function (defines your objective)\n",
    "\n",
    "* Originally introduced by Dempster, Laird, and Rubin in 1977 - ``Maximum Likelihood from Incomplete Data via the EM Algorithm''\n",
    "\n",
    "* EM is a method to simplify difficult maximum likelihood problems.\n",
    "\n",
    "* Suppose we observe $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ i.i.d. from $g(\\mathbf{x}_i | \\Theta)$\n",
    "\n",
    "* We want: $\\hat\\Theta = argmax  L(\\Theta|X) = argmax \\prod_{i=1}^N g(\\mathbf{x}_i | \\Theta)$\n",
    "\n",
    "* But suppose this maximization is very difficult.  EM simplifies it by expanding the problem to a bigger easier problem - ``demarginalization''\n",
    "\n",
    "\\begin{equation}\n",
    "g(x|\\Theta) = \\int_z f(x, z | \\Theta) dz\n",
    "\\end{equation}\n",
    "\n",
    "Main Idea: Do all of your analysis on $f$ and then integrate over the unknown z's.  \n",
    "\n",
    "### Censored Data Example\n",
    "* Suppose we observe $\\mathbf{y}_1, \\ldots, \\mathbf{y}_N$ i.i.d. from $f(\\mathbf{y} | \\Theta)$\n",
    "* Lets say that we know that values are censored at $\\ge a$\n",
    "* So, we see: $\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ (less than $a$) and we do not see $\\mathbf{y}_{m+1}, \\ldots, \\mathbf{y}_N$  which are censored and set to $a$. \n",
    "* Given this censored data, suppose we want to estimate the mean if the data was uncensored. \n",
    "* Our observed data likelihood in this case would be: \n",
    "\\begin{eqnarray}\n",
    "L &=& \\prod_{i=1}^m \\left[ 1 - F(a |\\theta)\\right]^{N-m}f(\\mathbf{y}_i | \\theta)\\\\\n",
    "&=& \\prod_{i=1}^m f(\\mathbf{y}_i | \\theta) \\prod_{j=m+1}^N \\int_a^\\infty f(\\mathbf{y}_j | \\theta) dy_j\n",
    "\\end{eqnarray}\n",
    "where $F(\\cdot)$ is the cumulative distribution function and $f(y|\\theta) = N(y|\\theta)$, for example. \n",
    "\n",
    "* So, the observed data likelihood would be very difficult to maximize to solve for $\\theta$\n",
    "* In EM, we introduce *latent variables* (i.e., ``hidden variables'') to simplify the problem\n",
    "* *The second step*: Define the *complete likelihood* by introducing variables that simplify the problem. \n",
    "\n",
    "* Going back to the censored data example, if we had observed the missing data, the problem would be easy to solve! It would simplify to a standard MLE.  For this example, the complete data likelihood is: \n",
    "\\begin{equation}\n",
    "L^c = \\prod_{i=1}^m f(y_i | \\theta) \\prod_{i=m+1}^N f(z_i | \\theta)\n",
    "\\end{equation}\n",
    "where $z_i$ are the latent, hidden variables. \n",
    "* Note: you cannot just use $a$ for the censored data, it would skew the results!\n",
    "* The complete data likelihood would be much much simplier to optimize for $\\theta$ if we had the $z$s... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading Assignment\n",
    "\n",
    "* Reading on K-Means: Section 9.1\n",
    "* Reading on GMMs and EM: Section 9.2-9.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
